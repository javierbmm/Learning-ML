{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learning_nltk.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javierbmm/Learning-ML/blob/master/learning_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfMpjoSvPQ4Q",
        "colab_type": "text"
      },
      "source": [
        "# Learning NLTK\n",
        "That's it. \n",
        "\n",
        "I want to process text. You want it too. Let's start.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_6Wp1brmd-z",
        "colab_type": "code",
        "outputId": "2e4d948e-570d-443c-948b-389c0f61f283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!apt-cache search urllib\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python-urllib3 - HTTP library with thread-safe connection pooling for Python\n",
            "python3-urllib3 - HTTP library with thread-safe connection pooling for Python3\n",
            "python-keepalive - urllib keepalive support for Python 2\n",
            "python-multipartposthandler - handler for urllib2 to enable multipart form uploading\n",
            "python-ndg-httpsclient - enhanced HTTPS support for httplib and urllib2 using PyOpenSSL for Python2\n",
            "python3-keepalive - urllib keepalive support for Python 3\n",
            "python3-ndg-httpsclient - enhanced HTTPS support for httplib and urllib2 using PyOpenSSL for Python3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij7OJm30PmbU",
        "colab_type": "text"
      },
      "source": [
        "# Downloading file  and unziping. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvACTXwkkAjS",
        "colab_type": "code",
        "outputId": "06823c6b-904b-4edd-a090-928569a7bf21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Download .zip folder\n",
        "from zipfile import ZipFile\n",
        "import urllib.request\n",
        "from tempfile import mktemp\n",
        "\n",
        "urllib.request.urlretrieve (\"http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\", \"trainingandtestdata.zip\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('trainingandtestdata.zip', <http.client.HTTPMessage at 0x7f392750a198>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddU-xfnpqRMl",
        "colab_type": "code",
        "outputId": "42ce31bf-7eb9-42f9-a325-2ad276f345cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# Unzip file  \n",
        "from zipfile import ZipFile \n",
        "  \n",
        "# specifying the zip file name \n",
        "file_name = \"trainingandtestdata.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode \n",
        "with ZipFile(file_name, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!') "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name                                             Modified             Size\n",
            "testdata.manual.2009.06.14.csv                 2010-03-04 20:20:12        74326\n",
            "training.1600000.processed.noemoticon.csv      2010-03-04 20:20:42    238803811\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPaac4VLPs_H",
        "colab_type": "text"
      },
      "source": [
        "# Loading in Pandas\n",
        "\n",
        "This section is actually not useful at all since we wont do Machine Learning processing in this colab. \n",
        "\n",
        "However, I didn't want to delete so, there it is. \n",
        "\n",
        "**Skip it if you want.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Hw2zAJgQqN",
        "colab_type": "code",
        "outputId": "8ce312ee-df42-4986-8fb5-f4a1132f9a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load data using Pandas\n",
        "import pandas as pd\n",
        "\n",
        "filename = 'training.1600000.processed.noemoticon.csv'\n",
        "names = ['polarity', 'id', 'date', 'query', 'user', 'text']\n",
        "data = pd.read_csv(filename, names=names, encoding = \"latin\")\n",
        "print(data.shape)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1600000, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAIGxe-xtwya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "005c6a75-5119-4078-de61-94327c293678"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity  ...                                               text\n",
              "0         0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1         0  ...  is upset that he can't update his Facebook by ...\n",
              "2         0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3         0  ...    my whole body feels itchy and like its on fire \n",
              "4         0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5_q4WV2QKT4",
        "colab_type": "text"
      },
      "source": [
        "# Let's process the text \n",
        "*Finally lmao.*\n",
        "\n",
        "We'll use NLTK for this. Pretty useful and straightforward library.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Processing text is done (mainly) in four steps:\n",
        "\n",
        "\n",
        "1.   Tokenize\n",
        "2.   Remove punctuation\n",
        "3.   Remove stopwords.\n",
        "4.   Stemming \n",
        "\n",
        "        or \n",
        "5. Lemmatizing.\n",
        "\n",
        "Note: It might be obvious but stemming and lemmatizing are different operations with similar results, so you must choose one of them.\n",
        "\n",
        "*I'm not going to get deep into these concepts since isn't the point of this colab and I'm lazy as f-.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWr561DtQexw",
        "colab_type": "text"
      },
      "source": [
        "# 1.   Tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyJEEVwGyj3P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "33260923-9818-4325-8651-6185338649dd"
      },
      "source": [
        "#Importing NLTK Library\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Extracting text from csv file\n",
        "reviews = data.text.str.cat(sep=' ')\n",
        "# Function to split text into word\n",
        "tokens = word_tokenize(reviews)\n",
        "# Setting a vocabulary of the words in tokens\n",
        "vocabulary = set(tokens)\n",
        "print(len(vocabulary))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "919703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtRbB3xL42tM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "ec701fb7-c575-4f94-9849-8e18c80e99b9"
      },
      "source": [
        "frequency_dist = nltk.FreqDist(tokens)\n",
        "sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['!',\n",
              " '.',\n",
              " '@',\n",
              " 'I',\n",
              " 'to',\n",
              " 'the',\n",
              " ',',\n",
              " 'a',\n",
              " 'i',\n",
              " '...',\n",
              " 'my',\n",
              " 'and',\n",
              " 'you',\n",
              " 'it',\n",
              " '?',\n",
              " 'is',\n",
              " 'for',\n",
              " 'in',\n",
              " 'of',\n",
              " \"'s\",\n",
              " \"n't\",\n",
              " ';',\n",
              " 'on',\n",
              " 'me',\n",
              " 'that',\n",
              " '&',\n",
              " 'have',\n",
              " \"'m\",\n",
              " 'so',\n",
              " ':',\n",
              " 'with',\n",
              " 'but',\n",
              " 'be',\n",
              " 'was',\n",
              " 'do',\n",
              " 'at',\n",
              " 'just',\n",
              " 'not',\n",
              " 'up',\n",
              " 'this',\n",
              " 'now',\n",
              " 'out',\n",
              " 'get',\n",
              " 'are',\n",
              " 'day',\n",
              " 'like',\n",
              " 'all',\n",
              " 'quot',\n",
              " 'good',\n",
              " 'http']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQpYnsAb5tJt",
        "colab_type": "text"
      },
      "source": [
        "# 2. Removing stopwords\n",
        "Above we can see that here is a lot of **unnecesary** words (noisy data).\n",
        "\n",
        "Let's get rid of them :D \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tlhI9IQ6OX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "cleaned_tokens = [w for w in tokens if not w in stop_words]\n",
        "#frequency_dist = nltk.FreqDist(cleaned_tokens)\n",
        "#sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SQQhxVbFKMc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "02633794-13a0-4b22-956e-c636fdccaff4"
      },
      "source": [
        "print(cleaned_tokens[0:50])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@', 'switchfoot', 'http', ':', '//twitpic.com/2y1zl', '-', 'Awww', ',', \"'s\", 'bummer', '.', 'You', 'shoulda', 'got', 'David', 'Carr', 'Third', 'Day', '.', ';', 'D', 'upset', 'ca', \"n't\", 'update', 'Facebook', 'texting', '...', 'might', 'cry', 'result', 'School', 'today', 'also', '.', 'Blah', '!', '@', 'Kenichan', 'I', 'dived', 'many', 'times', 'ball', '.', 'Managed', 'save', '50', '%', 'The']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN3GBrYQSWnJ",
        "colab_type": "text"
      },
      "source": [
        "# 3. Removing punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrgZ2eG_KZ8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "efc35401-c8fa-4caa-a772-38943e82ce68"
      },
      "source": [
        "# Importing punctuation\n",
        "import string \n",
        "print(string.punctuation)\n",
        "# Removing punctuation\n",
        "cleaned_tokens = [p for p in cleaned_tokens if not p in string.punctuation]\n",
        "\n",
        "print(cleaned_tokens[0:50])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "['switchfoot', 'http', '//twitpic.com/2y1zl', 'Awww', \"'s\", 'bummer', 'You', 'shoulda', 'got', 'David', 'Carr', 'Third', 'Day', 'D', 'upset', 'ca', \"n't\", 'update', 'Facebook', 'texting', '...', 'might', 'cry', 'result', 'School', 'today', 'also', 'Blah', 'Kenichan', 'I', 'dived', 'many', 'times', 'ball', 'Managed', 'save', '50', 'The', 'rest', 'go', 'bounds', 'whole', 'body', 'feels', 'itchy', 'like', 'fire', 'nationwideclass', \"'s\", 'behaving']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhtntqMNSaif",
        "colab_type": "text"
      },
      "source": [
        "# 4. Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF20rmPuLvfA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3b0f0da1-2fff-4663-dbcd-dc030585bf4d"
      },
      "source": [
        "# Do I need to explain this? It's Stemming! \n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "stemmed_tokens = [ps.stem(word) for word in cleaned_tokens]\n",
        "\n",
        "print(cleaned_tokens[0:50])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['switchfoot', 'http', '//twitpic.com/2y1zl', 'Awww', \"'s\", 'bummer', 'You', 'shoulda', 'got', 'David', 'Carr', 'Third', 'Day', 'D', 'upset', 'ca', \"n't\", 'update', 'Facebook', 'texting', '...', 'might', 'cry', 'result', 'School', 'today', 'also', 'Blah', 'Kenichan', 'I', 'dived', 'many', 'times', 'ball', 'Managed', 'save', '50', 'The', 'rest', 'go', 'bounds', 'whole', 'body', 'feels', 'itchy', 'like', 'fire', 'nationwideclass', \"'s\", 'behaving']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0R0y1ZGSZ5V",
        "colab_type": "text"
      },
      "source": [
        "# 5. Lemmatizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66kFeUbJS7GW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "793bbe6e-6da2-4f0c-f26a-bb42bcc67f13"
      },
      "source": [
        "# Yep. Lemmatizing.\n",
        "nltk.download('wordnet')\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "\n",
        "lemmatized_tokens = [wn.lemmatize(word) for word in cleaned_tokens]\n",
        "\n",
        "print(cleaned_tokens[0:50])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "['switchfoot', 'http', '//twitpic.com/2y1zl', 'Awww', \"'s\", 'bummer', 'You', 'shoulda', 'got', 'David', 'Carr', 'Third', 'Day', 'D', 'upset', 'ca', \"n't\", 'update', 'Facebook', 'texting', '...', 'might', 'cry', 'result', 'School', 'today', 'also', 'Blah', 'Kenichan', 'I', 'dived', 'many', 'times', 'ball', 'Managed', 'save', '50', 'The', 'rest', 'go', 'bounds', 'whole', 'body', 'feels', 'itchy', 'like', 'fire', 'nationwideclass', \"'s\", 'behaving']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}